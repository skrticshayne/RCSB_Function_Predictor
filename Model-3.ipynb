{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6833e9-3568-4719-8a14-cfee131310da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and reads the CSV file, combining the ec# columns to train/test for first 2 ec#s of each protein\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/carmenshero/Desktop/Crumble_Cookie/FINAL_ACTUALLALY_READY_TO_TRAIN.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Combine ec_first and ec_second into a single target column\n",
    "data['ec_combined'] = data['ec_first'].astype(str) + '.' + data['ec_second'].astype(str)\n",
    "\n",
    "# Display the first few rows to confirm successful loading and combination\n",
    "print(\"Data Loaded and EC Combined Successfully!\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e25016-0089-4103-9cd2-ce967913031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs the train-test split on the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and the combined target (y)\n",
    "X = data.drop(columns=['id', 'ec_first', 'ec_second', 'ec_combined'])  # Features\n",
    "y = data['ec_combined']  # Combined target\n",
    "\n",
    "\n",
    "# Perform the train-test split (80% train, 20% test split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shape of the splits\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc431ba-08d9-430d-ad24-dcb1fe68da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Let's goooo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize and train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Optional: Confusion matrix visualization\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)\n",
    "plt.title(\"Confusion Matrix for Random Forest\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce3f4b-f465-4a5e-a9d6-ef43cf1c6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch to find best hyperparameters for Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 250, 300, 400],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1_weighted',  # Metric for evaluation\n",
    "    n_jobs=-1  # Use all processors\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "print(\"Best F1-Weighted Score from GridSearchCV:\", grid_search.best_score_)\n",
    "\n",
    "# Update Random Forest Classifier with best parameters\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the tuned model\n",
    "y_pred_best_rf = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(\"Tuned Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best_rf))\n",
    "\n",
    "# Confusion matrix visualization for tuned model\n",
    "conf_matrix_best_rf = confusion_matrix(y_test, y_pred_best_rf)\n",
    "sns.heatmap(conf_matrix_best_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_rf_classifier.classes_, yticklabels=best_rf_classifier.classes_)\n",
    "plt.title(\"Confusion Matrix for Tuned Random Forest\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b7f5f-70f5-4862-a599-5dff2eb5260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which features impact the model the most\n",
    "# Feature importance from the best model\n",
    "importances = best_rf_classifier.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Optional: Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title(\"Feature Importance from Tuned Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3514a-2470-4bb4-a26e-c9a421f6aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tuned and untuned Random Forest models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature importance from the tuned model\n",
    "tuned_importances = best_rf_classifier.feature_importances_\n",
    "\n",
    "# Feature importance from the default model\n",
    "default_rf_classifier = RandomForestClassifier(random_state=42)  # Default RF\n",
    "default_rf_classifier.fit(X_train, y_train)  # Train default RF on the training set\n",
    "default_importances = default_rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Tuned Importance': tuned_importances,\n",
    "    'Default Importance': default_importances\n",
    "}).sort_values(by='Tuned Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importances (Comparison):\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Optional: Plot feature importance for both models\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='Tuned Importance',\n",
    "    y='Feature',\n",
    "    data=feature_importance_df,\n",
    "    color=\"blue\",\n",
    "    label=\"Tuned RF\"\n",
    ")\n",
    "sns.barplot(\n",
    "    x='Default Importance',\n",
    "    y='Feature',\n",
    "    data=feature_importance_df,\n",
    "    color=\"orange\",\n",
    "    alpha=0.6,\n",
    "    label=\"Default RF\"\n",
    ")\n",
    "plt.title(\"Feature Importance: Tuned vs. Default Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a70ca-f2aa-4afc-b79d-ffb2d5b3f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment to compare default and GridSearch-tuned hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Random states to test\n",
    "random_states = [0, 21, 42, 84, 123]\n",
    "\n",
    "# Initialize lists to store results\n",
    "default_results = []\n",
    "tuned_results = []\n",
    "\n",
    "# Loop over random states\n",
    "for state in random_states:\n",
    "    print(f\"\\n=== Testing with Random State: {state} ===\")\n",
    "    \n",
    "    # Default Random Forest\n",
    "    default_rf = RandomForestClassifier(random_state=state)\n",
    "    default_rf.fit(X_train, y_train)\n",
    "    y_pred_default = default_rf.predict(X_test)\n",
    "    default_report = classification_report(y_test, y_pred_default, output_dict=True)\n",
    "    default_results.append({\n",
    "        'Random State': state,\n",
    "        'Precision': default_report['weighted avg']['precision'],\n",
    "        'Recall': default_report['weighted avg']['recall'],\n",
    "        'F1-Score': default_report['weighted avg']['f1-score']\n",
    "    })\n",
    "\n",
    "    # Tuned Random Forest\n",
    "    tuned_rf = RandomForestClassifier(\n",
    "        random_state=state,\n",
    "        n_estimators=grid_search.best_params_['n_estimators'],\n",
    "        max_depth=grid_search.best_params_['max_depth'],\n",
    "        min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "        min_samples_leaf=grid_search.best_params_['min_samples_leaf']\n",
    "    )\n",
    "    tuned_rf.fit(X_train, y_train)\n",
    "    y_pred_tuned = tuned_rf.predict(X_test)\n",
    "    tuned_report = classification_report(y_test, y_pred_tuned, output_dict=True)\n",
    "    tuned_results.append({\n",
    "        'Random State': state,\n",
    "        'Precision': tuned_report['weighted avg']['precision'],\n",
    "        'Recall': tuned_report['weighted avg']['recall'],\n",
    "        'F1-Score': tuned_report['weighted avg']['f1-score']\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrames for easier analysis\n",
    "default_df = pd.DataFrame(default_results)\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDefault Random Forest Results:\")\n",
    "print(default_df)\n",
    "print(\"\\nTuned Random Forest Results:\")\n",
    "print(tuned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d5223-91fb-441f-b50d-ca37a42c6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot comparison of F1-Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(default_df['Random State'], default_df['F1-Score'], label='Default RF', marker='o')\n",
    "plt.plot(tuned_df['Random State'], tuned_df['F1-Score'], label='Tuned RF', marker='o')\n",
    "plt.title(\"Comparison of F1-Scores Across Random States\")\n",
    "plt.xlabel(\"Random State\")\n",
    "plt.ylabel(\"F1-Score (Weighted)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Compare precision and recall\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(default_df['Random State'], default_df['Precision'], label='Default RF Precision', marker='o')\n",
    "plt.plot(tuned_df['Random State'], tuned_df['Precision'], label='Tuned RF Precision', marker='o')\n",
    "plt.title(\"Comparison of Precision Across Random States\")\n",
    "plt.xlabel(\"Random State\")\n",
    "plt.ylabel(\"Precision (Weighted)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(default_df['Random State'], default_df['Recall'], label='Default RF Recall', marker='o')\n",
    "plt.plot(tuned_df['Random State'], tuned_df['Recall'], label='Tuned RF Recall', marker='o')\n",
    "plt.title(\"Comparison of Recall Across Random States\")\n",
    "plt.xlabel(\"Random State\")\n",
    "plt.ylabel(\"Recall (Weighted)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f9174-1184-4dc3-be44-3519b5258d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
